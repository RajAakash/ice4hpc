# -*- coding: utf-8 -*-
"""util.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYmmhXqH5U-rLKw-lw_fTdG27Qxb_y8H
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation, Dropout, Flatten, Input, Dense, concatenate
from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error
from tensorflow.keras.models import Model, Sequential, load_model
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import os
import sys
import time
#import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from numpy import random
from scipy.fft import fft, ifft, fftfreq
import copy
import scipy
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import random
import os
import sys
import time
#import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from numpy import random
from scipy.fft import fft, ifft, fftfreq
import copy
import scipy
from sklearn.model_selection import train_test_split
import yaml
#import optuna
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf
import random
from sklearn.ensemble import RandomForestRegressor
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation, Dropout, Flatten, Input, Dense, concatenate
from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error, mean_absolute_percentage_error
from tensorflow.keras.models import Model, Sequential, load_model, model_from_json
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
#import preprocessing
#import dataloader
#from preprocessing import preprocessor
#from dataloader import dataLoader
import sys
import os
import os.path
import csv
#import ensemRegressor
#import optunatransformator1
import math
import keras
from be_great import GReaT
#import torch
"""
class deltaUQ(keras.Model):
  def call(self, inputs):
      x = self.block_1(inputs)
      x = self.block_2(x)
      x = self.global_pool(x)
      return self.classifier(x)

  def set_parms(self, anchors=None, n_anchors=1, return_std=False):
    #super(deltaUQ, self).__init__()
    self.anchors = anchors
    # best_weights to store the weights at which the minimum loss occurs.
    self.n_anchors = n_anchors
    self.return_std = return_std
  def create_anchored_batch(self,x,anchors=None,n_anchors=1,corrupt=False):
    '''
    anchors (default=None):
    if passed, will use the same set of anchors for all batches during training.
    if  None, we will use a shuffled input minibatch to forward( ) as anchors for that batch (random)

    During *inference* it is recommended to keep the anchors fixed for all samples.

    n_anchors is chosen as min(n_batch,n_anchors)
    '''
    print("x")
    print(x)
    n_samples = x.shape[0]
    if anchors is None:
      #anchors = x[np.random.randint(n_samples, size=(n_samples,)),:]
      anchors =  tf.random.shuffle(x, seed=None, name=None)
      anchors = anchors.numpy()

    ## make anchors (n_anchors) --> n_img*n_anchors
    print("Num of samples")
    print(n_samples)
    print("*****************Anchors***************")
    print(anchors)
    print(anchors.shape)
    if self.training:
      A = anchors[np.random.randint(anchors.shape[0],(n_samples*n_anchors,)),:]
    else:
      anchors =  tf.random.shuffle(anchors, seed=None, name=None)
      anchors = anchors.numpy()
      A = np.repeat(anchors[0:n_anchors,:], n_samples, axis=0)

    if corrupt:
      refs = self.corruption(A)
    else:
      refs = A

    ## before computing residual, make minibatch (n_img) --> n_img* n_anchors
    if len(x.shape)<=2:
      #diff = x.tile((n_anchors,1))
      diff = np.tile(x, (n_anchors,1))
      assert diff.shape[1]==A.shape[1], f"Tensor sizes for `diff`({diff.shape}) and `anchors` ({A.shape}) don't match!"
      diff -= A
    else:
      diff = np.tile(x, (n_anchors,1,1,1)) - A
      #diff = x.tile((n_anchors,1,1,1)) - A

    #batch = torch.cat([refs,diff],axis=1)
    batch = np.concatenate((refs, diff), axis=1)
    return batch

  def train_step(self, data):
    # Unpack the data. Its structure depends on your model and
    # on what you pass to `fit()`.
    x, y = data
    a_batch = self.create_anchored_batch(x, anchors=self.anchors,n_anchors=self.n_anchors)
    with tf.GradientTape() as tape:
      y_pred = self(x, training=True)  # Forward pass
      # Compute the loss value
      # (the loss function is configured in `compile()`)
      y_pred = y_pred.reshape(self.n_anchors, x.shape[0], y_pred.shape[1])
      y_pred = y_pred.mean(0)
      loss = self.compute_loss(y=y, y_pred=y_pred)

    # Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    # Update metrics (includes the metric that tracks the loss)
    for metric in self.metrics:
      if metric.name == "loss":
        metric.update_state(loss)
      else:
        metric.update_state(y, y_pred)
    # Return a dict mapping metric names to current value
    return {m.name: m.result() for m in self.metrics}
"""
def create_deltaUQ(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment = 0.5, actF="relu", lossF="mean_squared_error", transfer=False, frozen_layers = 0, num_of_outputs = 1):
  inputs = keras.Input(103,)
  for i in range(num_of_layers_1):
    inputs = Dense(units=neurons_input, activation=actF)(inputs)
    inputs = BatchNormalization(momentum=moment)(inputs)
    #model.add(Dropout(0.4)
  outputs = Dense(units=num_of_outputs)(inputs)
  model = deltaUQ(inputs, outputs)
  #, kernel_regularizer=tf.keras.regularizers.l1(0.01),
  #                            activity_regularizer=tf.keras.regularizers.l2(0.01)))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model


def dataGen(generator, src_columns, tar_columns, target_data):
  list_of_dfs = []
  for column in tar_columns.columns:
    tr1 = pd.concat([src_columns, tar_columns[[column]]], axis=1)
    generator.fit(tr1[0:20])
    temp = target_data
    temp[column]= float("NAN") #np.nan
    imputed_data = generator.impute(temp, max_length=200)
    list_of_dfs.append(imputed_data[[column]])
  i = 0
  for column in tar_columns.columns:
    target_data = pd.concat([target_data, list_of_dfs[i]], axis=1)
    i = i + 1
  return target_data
def create_model4(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment = 0.5, actF="relu", lossF="mean_squared_error", transfer=False, frozen_layers = 0, num_of_outputs = 1):

  model = Sequential()
  for i in range(num_of_layers_1):
    layer = Dense(units=neurons_input, activation=actF)
    layer2 = BatchNormalization(momentum=moment)
    if transfer == True and i< frozen_layers:
      #(num_of_layers_1-1):
      layer.trainable=False
      layer2.trainable=False
    model.add(layer)
    model.add(layer2)
    #model.add(Dropout(0.4))
  finalLayer = Dense(units=num_of_outputs)
  #finalLayer.trainable=False
  model.add(finalLayer)
  #, kernel_regularizer=tf.keras.regularizers.l1(0.01),
  #                            activity_regularizer=tf.keras.regularizers.l2(0.01)))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model

class EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):
  """Stop training when the loss is at its min, i.e. the loss stops decreasing.

  Arguments:
      patience: Number of epochs to wait after min has been hit. After this
      number of no improvement, training stops.
  """
  def __init__(self, patience=0, arg_loss="val_loss"):
    super(EarlyStoppingAtMinLoss, self).__init__()
    self.patience = patience
    # best_weights to store the weights at which the minimum loss occurs.
    self.best_weights = None
    self.targeted_loss = arg_loss

  def on_train_begin(self, logs=None):
    # The number of epoch it has waited when loss is no longer minimum.
    self.wait = 0
    # The epoch the training stops at.
    self.stopped_epoch = 0
    # Initialize the best as infinity.
    self.best = np.Inf

  def on_epoch_end(self, epoch, logs=None):
    current = logs.get(self.targeted_loss)
    if np.less(current, self.best):
      self.best = current
      self.wait = 0
      # Record the best weights if current results is better (less).
      self.best_weights = self.model.get_weights()
    else:
      self.wait += 1
      if self.wait >= self.patience:
        self.stopped_epoch = epoch
        self.model.stop_training = True
        print("Restoring model weights from the end of the best epoch.")
        self.model.set_weights(self.best_weights)

  def on_train_end(self, logs=None):
    if self.stopped_epoch > 0:
      print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))

def sampleMaker(tar_x_scaled, tar_y_scaled, path, n):
  os.makedirs(os.path.dirname(path), exist_ok=True)
  indices = open(path, "w" )
  readModeOn = False
  dropIndices = []
  x2 =[]
  lb2 = []
  totallen = len(tar_x_scaled)
  if isinstance(n, float):
    num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
  else:
    num_of_samples = n
  for k in range(num_of_samples):
    index = random.randint(0, totallen-1 ) #int(nums[i])
    while index in dropIndices:
      index = random.randint(0, totallen-1 )
    indices.write(str(index))
    indices.write(" ")
    indices.write("\n")
    #print(index)
    dropIndices.append(index)
    if len(tar_x_scaled[index:index+1])> 0:
      x2.append(tar_x_scaled[index:index+1])
      lb2.append(tar_y_scaled[index:index+1])
  #dropIndices = list([*set(dropIndices)])
  #print(f"after removing duplicates dropIndices {dropIndices}")
  drop_len = len(dropIndices)
  print(f"drop indices length {drop_len}")
  print(f"num of samples needed to be added {num_of_samples}")
  for k in range(drop_len):
    idx = dropIndices.pop(0)
    for d in range(len(dropIndices)):
      if dropIndices[d]> idx:
        dropIndices[d] -= 1
    #print(f" dropIndices {dropIndices}")
    #print(f" len of tar_x_scaled {len(tar_x_scaled)}")
    #print(f"idx {idx}")
    tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
    tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
  indices.close()
  """
  print("inside fine tuning x2 shape")
  print(f"x2 shape {x2.shape}")
  """
  print(x2)
  """
  print(f"x2 shape {x2.shape}")
  """
  x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
  lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
  x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
  lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled


def n_sampleMaker(tar_x_scaled, tar_y_scaled, path, n):
  os.makedirs(os.path.dirname(path), exist_ok=True)
  indices = open(path, "w" )
  readModeOn = False
  dropIndices = []
  x2 =[]
  lb2 = []
  totallen = len(tar_x_scaled)
  if isinstance(n, float):
    num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
  else:
    num_of_samples = n
  for k in range(num_of_samples):
    index = random.randint(0, totallen-1 ) #int(nums[i])
    while index in dropIndices:
      index = random.randint(0, totallen-1 )
    indices.write(str(index))
    indices.write(" ")
    indices.write("\n")
    #print(index)
    dropIndices.append(index)
    if len(tar_x_scaled[index:index+1])> 0:
      x2.append(tar_x_scaled[index:index+1])
      lb2.append(tar_y_scaled[index:index+1])
  #dropIndices = list([*set(dropIndices)])
  #print(f"after removing duplicates dropIndices {dropIndices}")
  drop_len = len(dropIndices)
  print(f"drop indices length {drop_len}")
  print(f"num of samples needed to be added {num_of_samples}")
  for k in range(drop_len):
    idx = dropIndices.pop(0)
    for d in range(len(dropIndices)):
      if dropIndices[d]> idx:
        dropIndices[d] -= 1
    #print(f" dropIndices {dropIndices}")
    #print(f" len of tar_x_scaled {len(tar_x_scaled)}")
    #print(f"idx {idx}")
    tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
    tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
  indices.close()
  print("inside fine tuning x2 shape")
  print(f"x2 shape {x2.shape}")
  print(x2)
  print(f"x2 shape {x2.shape}")
  x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
  lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
  x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
  lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled



def sampleLoader(tar_x_scaled, tar_y_scaled, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(tar_x_scaled)
    print(f"n is {n}")
    if isinstance(n, float):
      num_of_samples = int(math.ceil(n*len(tar_x_scaled)))  #int(math.ceil(n*len(tar_x_scaled)))
    else:
      num_of_samples = n
    print(f"len of tar_x_scaled {len(tar_x_scaled)}")
    print(f"num of samples {num_of_samples}")
    loaded_indices = indices.readlines()
    print(f"loaded indices {loaded_indices}")
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      print(f"index is {index}")
      dropIndices.append(index)
      if len(tar_x_scaled[index:index+1])> 0:
        x2.append(tar_x_scaled[index:index+1])
        lb2.append(tar_y_scaled[index:index+1])
    dropIndices = list([*set(dropIndices)])
    drop_len = len(dropIndices)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
      tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
    indices.close()
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    # print(f"x2 {x2}")
    #print(f"x2 shape {x2.shape}")
    x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
    lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
    x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
    lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled



def n_sampleLoader(tar_x_scaled, tar_y_scaled, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(tar_x_scaled)
    print(f"n is {n}")
    if isinstance(n, float):
      num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
    else:
      num_of_samples = n
    print(f"len of tar_x_scaled {len(tar_x_scaled)}")
    print(f"num of samples {num_of_samples}")
    loaded_indices = indices.readlines()
    print(f"loaded indices {loaded_indices}")
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      print(f"index is {index}")
      dropIndices.append(index)
      if len(tar_x_scaled[index:index+1])> 0:
        #x2.append(tar_x_scaled[index:index+1])
        lb2.append(tar_y_scaled[index:index+1])
    dropIndices = list([*set(dropIndices)])
    x2 = tar_x_scaled.iloc[dropIndices]
    drop_len = len(dropIndices)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    tar_x_scaled = tar_x_scaled.drop(tar_x_scaled.index[dropIndices])
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      #tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
      tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
    indices.close()
    print(f"x2 {x2}")
    print(f"x2 shape {x2.shape}")
    x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
    lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
    print(f"after converting to tensor x2 shape {x2.shape}")
    #x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
    lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled


def ript_sampleLoader(tar_x_scaled, tar_y_scaled, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(tar_x_scaled)
    print(f"n is {n}")
    if isinstance(n, float):
      num_of_samples = int(math.ceil(n*len(tar_x_scaled)))
    else:
      num_of_samples = n
    print(f"len of tar_x_scaled {len(tar_x_scaled)}")
    print(f"num of samples {num_of_samples}")
    loaded_indices = indices.readlines()
    print(f"loaded indices {loaded_indices}")
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      print(f"index is {index}")
      dropIndices.append(index)
      if len(tar_x_scaled[index:index+1])> 0:
        #x2.append(tar_x_scaled[index:index+1])
        lb2.append(tar_y_scaled[index:index+1])
    dropIndices = list([*set(dropIndices)])
    x2 = tar_x_scaled.iloc[dropIndices]
    drop_len = len(dropIndices)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    #tar_x_scaled = tar_x_scaled.drop(dropIndices)
    tar_x_scaled = tar_x_scaled.drop(tar_x_scaled.index[dropIndices])
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      #tar_x_scaled = np.delete(tar_x_scaled, idx, 0)
      tar_y_scaled = np.delete(tar_y_scaled, idx, 0)
    indices.close()
    print(f"x2 {x2}")
    print(f"x2 shape {x2.shape}")
    #x2 = tf.convert_to_tensor( x2, dtype=tf.float64)
    lb2 = tf.convert_to_tensor( lb2, dtype=tf.float64)
    print(f"after converting to tensor x2 shape {x2.shape}")
    #x2 = tf.reshape(x2, (x2.shape[0], x2.shape[2]))
    lb2 = tf.reshape(lb2, (lb2.shape[0], lb2.shape[2]))
  return x2, lb2, tar_x_scaled, tar_y_scaled



def tfSampleLoader(target_dataframe, path, n):
  dropIndices = []
  x2 =[]
  lb2 = []
  if os.path.isfile(path):
    indices = open(path, "r" )
    totallen = len(target_dataframe)
    if isinstance(n, float):
      num_of_samples = int(n*len(target_dataframe))
    else:
      num_of_samples = n
    loaded_indices = indices.readlines()
    for k in range(num_of_samples):
      index =  int(loaded_indices[k]) #
      dropIndices.append(index)
      if len(target_dataframe[index:index+1])> 0:
        #if k==0:
        x2.append(target_dataframe[index:index+1].values)
        #else:
        #x2 = pd.concat([x2, target_dataframe[index:index+1]], axis=1)
    print(f"x2 {x2}")
    dropIndices = list([*set(dropIndices)])
    drop_len = len(dropIndices)
    x2 = np.reshape(x2, (drop_len, 18))
    print(f"modified x2 {x2}")
    x2 = pd.DataFrame(x2, columns=target_dataframe.columns)
    print(f"dropindices is {dropIndices}")
    print(f" drop len is {drop_len}")
    fi = dropIndices[0]
    print(f"first sample is {target_dataframe[fi:fi+1]}")
    for k in range(drop_len):
      idx = dropIndices.pop(0)
      for d in range(len(dropIndices)):
        if dropIndices[d]> idx:
          dropIndices[d] -= 1
      target_dataframe.drop(target_dataframe.index[idx], axis=0, inplace=True)
      #target_dataframe = np.delete(target_dataframe, idx, 0)
    indices.close()

    tuning_samples = x2 #target_dataframe.iloc[dropIndices]
    training_samples = target_dataframe #.drop(dropIndices, axis=0)
  return tuning_samples, training_samples


def sorceModelLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, model_path, model_weights, output_n=1):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  if build_now:
    loaded_model = create_model4(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers, num_of_outputs = output_n)
    if len(source_features)<10:
      print("Building model based on training loss")
      callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
    else:
      print("Building model based on validation loss")
      callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    if save_bit:
      os.makedirs(os.path.dirname(model_path), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(model_path, "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(model_weights)
      print("Saved model to disk")
      json_file.close()
  else:
    #model.load_weights(source_model_weights)
    json_file = open(model_path, 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(model_weights)
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model





def sourceModelLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, output_n=1):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  if build_now:
    loaded_model = create_model4(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers, num_of_outputs = output_n)
    if len(source_features)<10:
      print("Building model based on training loss")
      callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
    else:
      print("Building model based on validation loss")
      callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    if save_bit:
      os.makedirs(os.path.dirname(os.getcwd()+global_config['source_model']), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(os.getcwd()+global_config['source_model'], "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(os.getcwd()+global_config['source_model_weights'])
      print("Saved model to disk")
      json_file.close()
  else:
    #model.load_weights(source_model_weights)
    json_file = open(os.getcwd()+global_config['source_model'], 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(os.getcwd()+global_config['source_model_weights'])
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model
def deltaUQSourceLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, anchors, n_anchors, output_n=1):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  h = source_features
  make_anchors = lambda h: h.min()+torch.rand(1000,1)*(h.max()-h.min())
  anchors = make_anchors(h)
  if build_now:
    loaded_model = create_deltaUQ(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers, num_of_outputs = output_n)
    loaded_model.set_parms(anchors=anchors, n_anchors=n_anchors, return_std=False)
    loaded_model.training=True
    print("source features")
    print(source_features)
    if len(source_features)<10:
      print("Building model based on training loss")
      callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
    else:
      print("Building model based on validation loss")
      callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    if save_bit:
      os.makedirs((os.getcwd()+global_config['source_model']), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(global_config['source_model'], "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(os.getcwd()+global_config['source_model_weights'])
      print("Saved model to disk")
      json_file.close()
  else:
    #model.load_weights(source_model_weights)
    json_file = open(global_config['source_model'], 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(global_config['source_model_weights'])
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model
def targetModelLoader(source_features, source_labels, transfer_permission, num_of_frozen_layers, build_now, global_config, target_app, save_bit, callback_patience, train_epoch):
  
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  
  #nums = global_config['target_model_params']
  callback2 =  tf.keras.callbacks.EarlyStopping(monitor='loss', patience=40)
  batch_size = int(nums[3])
  if batch_size> len(source_features):
    batch_size = None
  #model.fit(source_features, source_labels, batch_size = int(nums[3]) , epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True)
  if build_now:
    loaded_model = create_model4(neurons_input = int(nums[0]) , num_of_layers_1= int(nums[1]), lr= float(nums[2]) , moment = float(nums[4]) , actF="relu", lossF="mean_squared_error", transfer=transfer_permission, frozen_layers= num_of_frozen_layers)
    #if len(source_features)<10:
    print("Building model based on training loss")
    callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=callback_patience)# EarlyStoppingAtMinLoss(patience = callback_patience, arg_loss="loss") #
    loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs= train_epoch, callbacks=[ callback2])
    """
    else:
      print("Building model based on validation loss")
      callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=callback_patience) #EarlyStoppingAtMinLoss(patience= callback_patience, arg_loss="val_loss")
      loaded_model.fit(source_features, source_labels, batch_size = batch_size, epochs= train_epoch, callbacks=[ callback2], validation_split=0.2, shuffle= True )
    #loaded_model.fit(source_features, source_labels, batch_size = batch_size , epochs=1000, callbacks=[ callback2]) #, validation_split=0.2, shuffle= True)
    #model.save_weights(source_model_weights)
    
    if save_bit:
      os.makedirs(os.path.dirname(global_config['source_model']), exist_ok=True)
      model_json = loaded_model.to_json()
      with open(global_config['source_model'], "w") as json_file:
        json_file.write(model_json)
      # serialize weights to HDF5
      loaded_model.save_weights(global_config['source_model_weights'])
      print("Saved model to disk")
      json_file.close()
    """
  else:
    #model.load_weights(source_model_weights)
    json_file = open(global_config['source_model'], 'r')
    loaded_model_json = json_file.read()

    json_file.close()
    loaded_model = model_from_json(loaded_model_json)
    # load weights into new model
    loaded_model.load_weights(global_config['source_model_weights'])
    print("Loaded model from disk")
    i = 0
    if transfer_permission == True:
      while i< (2*num_of_frozen_layers):
        loaded_model.layers[i].trainable=False
        loaded_model.layers[i].trainable=False
        i += 2
  opt1 = tf.keras.optimizers.Nadam(learning_rate=float(nums[2]))
  loaded_model.compile(loss='mean_squared_error', optimizer= opt1, metrics=['mse'])
  print("Model Building Done")
  return loaded_model
def update_source_model(source_model, target_features, target_labels, fine_tune_sample_features, fine_tune_sample_labels, global_config, epoch_num):
  f = open(os.getcwd()+global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  batch_size = int(nums[3])
  predictions0 = source_model.predict(target_features)
  mse0 = mean_squared_error(target_labels, predictions0)
  mape0 = mean_absolute_percentage_error(target_labels, predictions0)
  print("fine tune features")
  print(fine_tune_sample_features)
  print("fine tune labels")
  print(fine_tune_sample_labels)
  fine_tune_sample_labels = np.reshape(fine_tune_sample_labels,(-1,1))
  if batch_size> len(fine_tune_sample_features):
    batch_size = None
  if len(fine_tune_sample_features)<10:
    """
    ls = len(fine_tune_sample_features)
    if ls<=2:
      epoch_num = 20
    elif ls>2 and ls<=7:
      epoch_num = 50
    elif ls >8 and ls<=10:
      epoch_num = 100
    """
    #epoch_num=20
    callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=epoch_num, callbacks=[ callback2])
  else:
    callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
  predictions2 = source_model.predict(target_features)
  mse2 = mean_squared_error(target_labels, predictions2)
  mape2 = mean_absolute_percentage_error(target_labels, predictions2)
  return mse0, mse2, mape0, mape2

def getupdate_source_model(source_model, target_features, target_labels, fine_tune_sample_features, fine_tune_sample_labels, global_config):
  f = open(global_config['source_model_params'], "r")
  nums = f.readlines()
  nums = nums[0].split(" ")
  f.close()
  batch_size = int(nums[3])
  predictions0 = source_model.predict(target_features)
  mse0 = mean_squared_error(target_labels, predictions0)
  if batch_size> len(fine_tune_sample_features):
    batch_size = None
  if len(fine_tune_sample_features)<10:
    callback2 = EarlyStoppingAtMinLoss(patience = 40, arg_loss="loss") #
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2])
  else:
    callback2 = EarlyStoppingAtMinLoss(patience=40, arg_loss="val_loss")
    source_model.fit(fine_tune_sample_features, fine_tune_sample_labels, batch_size = batch_size, epochs=1000, callbacks=[ callback2], validation_split=0.2, shuffle= True )
  predictions2 = source_model.predict(target_features)
  mse2 = mean_squared_error(target_labels, predictions2)
  return mse0, mse2, source_model

def yaml_key_adder(config, key, value):
  yml_config =f"{key}: {value}"
  with open(config, "a") as f:
    f.write(yml_config)
  f.close()
def scatterPlot(ground_truth, predictions, saving_path, title):
  os.makedirs(os.path.dirname(saving_path), exist_ok=True)
  matplotlib.rcParams['mathtext.fontset'] = 'stix'
  matplotlib.rcParams['font.family'] = 'STIXGeneral'
  plt.figure(figsize=(10,10))
  plt.scatter(ground_truth, predictions.ravel())
  mse,r2 = mean_squared_error(ground_truth#.reshape(-1,1)
                , predictions), r2_score(ground_truth#.reshape(-1,1)
                                , predictions)
  plt.xlim([0, 1]) 
  #plt.ylim([-5, 20]) 
  plt.xlabel('Actual Labels', fontsize=32)
  plt.ylabel('Predicted Labels', fontsize=32)
  plt.xticks(fontsize=32)
  plt.yticks(fontsize=32)
  plt.title(f'MSE: {mse}, R2: {r2}-{title}')
  plt.savefig(saving_path)


def dataToCsv(data, csv_filename):
  os.makedirs(os.path.dirname(csv_filename), exist_ok=True)
  fileI = open(csv_filename, "w")
  writerI = csv.writer(fileI)
  for a in data:
    writerI.writerow(a)
  fileI.close()
def newscatterPlot(ground_truth, predictions, saving_path, ground_truth_filename, predictions_filename, title):
  os.makedirs(os.path.dirname(saving_path), exist_ok=True)
  os.makedirs(os.path.dirname(ground_truth_filename), exist_ok=True)
  fileI = open(ground_truth_filename, "w")
  writerI = csv.writer(fileI)
  #ground_truth = list(ground_truth)
  #predictions = list(predictions)
  for a in ground_truth:
    row = []
    row.append(a)
    writerI.writerow(row)
  fileI.close()
  os.makedirs(os.path.dirname(predictions_filename), exist_ok=True)
  fileI = open(predictions_filename, "w")
  writerI = csv.writer(fileI)
  for a in predictions:
    row = []
    row.append(a)
    writerI.writerow(row)
  fileI.close()
  matplotlib.rcParams['mathtext.fontset'] = 'stix'
  matplotlib.rcParams['font.family'] = 'STIXGeneral'
  plt.figure(figsize=(10,10))
  plt.scatter(ground_truth, predictions)
  mse,r2 = mean_squared_error(ground_truth#.reshape(-1,1)
                , predictions), r2_score(ground_truth#.reshape(-1,1)
                                , predictions)
  plt.xlim([0, 1])
  #plt.ylim([-5, 20]) 
  plt.xlabel('Actual Labels', fontsize=32)
  plt.ylabel('Predicted Labels', fontsize=32)
  plt.xticks(fontsize=32)
  plt.yticks(fontsize=32)
  plt.title(f'MSE: {mse}, R2: {r2}-{title}')
  plt.savefig(saving_path)


def plot_graphs(history, string, filename):
  os.makedirs(os.path.dirname(filename), exist_ok=True)
  matplotlib.rcParams['mathtext.fontset'] = 'stix'
  matplotlib.rcParams['font.family'] = 'STIXGeneral'
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xticks(fontsize=32)
  plt.yticks(fontsize=32)
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.savefig(filename)

#
