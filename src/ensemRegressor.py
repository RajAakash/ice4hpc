# -*- coding: utf-8 -*-
"""SubSpaceJS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13TKGc6sVjhS5Ok9MTliNmm8aGU8X4xX8
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

import random
import sys
import time
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as keras_backend
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import concatenate
from sklearn.feature_selection import mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from operator import add
from numpy import asarray
import math
import scipy
import seaborn as sns
#import networkx as nx
import numpy as np
import optuna as optuna
import json

class EarlyStoppingAtMinLoss_k_reg(tf.keras.callbacks.Callback):
  """Stop training when the loss is at its min, i.e. the loss stops decreasing.

  Arguments:
      patience: Number of epochs to wait after min has been hit. After this
      number of no improvement, training stops.
  """
  def __init__(self, patience=0, arg_loss="val_loss"):
    super(EarlyStoppingAtMinLoss_k_reg, self).__init__()
    self.patience = patience
    # best_weights to store the weights at which the minimum loss occurs.
    self.best_weights = None
    self.targeted_loss = arg_loss

  def on_train_begin(self, logs=None):
    # The number of epoch it has waited when loss is no longer minimum.
    self.wait = 0
    # The epoch the training stops at.
    self.stopped_epoch = 0
    # Initialize the best as infinity.
    self.best = np.Inf

  def on_epoch_end(self, epoch, logs=None):
    current = logs.get(self.targeted_loss)
    if np.less(current, self.best):
      self.best = current
      self.wait = 0
      # Record the best weights if current results is better (less).
      self.best_weights = self.model.get_weights()
    else:
      self.wait += 1
      if self.wait >= self.patience:
        self.stopped_epoch = epoch
        self.model.stop_training = True
        print("Restoring model weights from the end of the best epoch.")
        self.model.set_weights(self.best_weights)

  def on_train_end(self, logs=None):
    if self.stopped_epoch > 0:
      print("Epoch %05d: early stopping" % (self.stopped_epoch + 1))


##################### Model Creation
def create_model(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment =0.8, actF="relu", lossF="mean_squared_error"):

  model = Sequential()
  for i in range(num_of_layers_1):
    model.add(Dense(units=neurons_input, activation=actF))
    model.add(BatchNormalization(momentum=moment))
    #model.add(Dropout(0.6))
  model.add(Dense(units=1))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model
def create_model2(
                 neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment =0.8, actF="relu", lossF="mean_squared_error", input_dim=16):

  model = Sequential()
  model.add(tf.keras.Input(shape=(input_dim,)))
  for i in range(num_of_layers_1):
    model.add(Dense(units=neurons_input, activation=actF))
    model.add(BatchNormalization(momentum=moment))
    #model.add(Dropout(0.6))
  opt1 = tf.keras.optimizers.Nadam(learning_rate=lr)
  model.compile(loss=lossF, optimizer=opt1, metrics=['mse','mae','mape'])
  return model
def custom_loss(y_true, y_pred):
  # calculating squared difference between target and predicted values
  miu = tf.reduce_mean(y_pred, 1)  
  reshaped_mean = tf.reshape(miu, (len(miu), 1)) 
  diversity_error = tf.square(tf.subtract(y_pred , reshaped_mean))
  individual_error = tf.square(y_pred - y_true)  
  loss = individual_error - diversity_error
  #tf.print(len(loss))
  return loss
def compute_rse(y,yhat):
  #y = y.ravel()
  #yhat = yhat.ravel()
  yhat = tf.cast(yhat, dtype=tf.float64)
  mu = np.mean(y)
  return np.sqrt(np.sum((y-yhat)**2))/np.sqrt(np.sum((y-mu)**2))
class EnsembleOfRegressor():
  def __init__(self, neurons_input = 1, num_of_layers_1=1,
                  lr=0.01, moment =0.8, actF="relu", lossF="mean_squared_error", input_dim=29, num_of_regressors = 10):
    super(EnsembleOfRegressor, self).__init__()
    self.feature_extractor = create_model2(neurons_input, num_of_layers_1, lr, moment, actF, lossF, input_dim)
    self.regressors =[]
    self.num_of_regressors = num_of_regressors
    for i in range(self.num_of_regressors):
      demo = create_model(neurons_input, num_of_layers_1, lr, moment, actF, lossF)
      self.regressors.append(demo)
    #feature_extractor.trainable(True)
    intermediate = Flatten()(self.feature_extractor.layers[-1].output)
    #outputs = tf.concat([relu_out, linear_out], axis=1)
    for i in range(self.num_of_regressors):
      if i==0:
        output = self.regressors[i](intermediate)
      else:
        #output  = output + regressors[i](intermediate)
        output  = tf.concat([output, self.regressors[i](intermediate)], axis =1)
    self.model = tf.keras.Model(self.feature_extractor.inputs, output)
    self.model.compile(optimizer='nadam', loss=custom_loss, metrics="mse")
  def getModel(self):
    return self.model
  def getBestRegressor(self, Input, labels):
    lowestError = 0
    predictions = self.model.predict(Input)
    regressorWisePredictions = tf.transpose(predictions)
    for i in range(self.num_of_regressors):
      if i == 0:
        lowestError = compute_rse(labels, regressorWisePredictions[i])
        self.bestRegressor =0
      else:
        if lowestError>compute_rse(labels, regressorWisePredictions[i]):
          lowestError = compute_rse(labels, regressorWisePredictions[i])
          self.bestRegressor = i
    return self.bestRegressor
  def fitData(self, Input, Labels, Epochs, Batch_size, loss="val_loss"):
    callback1 = EarlyStoppingAtMinLoss_k_reg(patience = 10, arg_loss=loss)
    if loss =="loss":
      print(f"Input {Input}")
      print(f"Labels {Labels}")
      self.model.fit(Input, Labels, epochs=Epochs, batch_size=Batch_size, callbacks = [callback1])
    else:
      self.model.fit(Input, Labels, epochs=Epochs, batch_size=Batch_size, callbacks = [callback1], validation_split=0.2, shuffle= True)

  def prediction(self, Input):
    predictions = self.model.predict(Input)
    regressorWisePredictions = tf.transpose(predictions)
    return regressorWisePredictions[self.bestRegressor]                            
